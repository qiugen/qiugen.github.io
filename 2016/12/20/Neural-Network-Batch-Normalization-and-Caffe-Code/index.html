<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="enjoy"><title>浅谈Batch Normalization及其Caffe实现 | 水滴石穿</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">浅谈Batch Normalization及其Caffe实现</h1><a id="logo" href="/.">水滴石穿</a><p class="description">探索，保持渴望，无所畏惧</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">浅谈Batch Normalization及其Caffe实现</h1><div class="post-meta">Dec 20, 2016<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/" href="/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#Motivation"><span class="toc-number">1.</span> <span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Normalization-via-Mini-Batch-Statistics"><span class="toc-number">2.</span> <span class="toc-text">Normalization via Mini-Batch Statistics</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Implementation-in-Caffe"><span class="toc-number">3.</span> <span class="toc-text">Implementation in Caffe</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#总结"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#参考文献"><span class="toc-number">5.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div><div class="post-content"><p><strong>&#x6458;&#x8981;:</strong>2015&#x5E74;2&#x6708;&#x4EFD;&#xFF0C;Google&#x548C;MSRA&#x7684;paper&#x76F8;&#x7EE7;&#x5728;<a href="https://arxiv.org/" target="_blank" rel="external">arxiv.org</a>&#x4E0A;&#x6A2A;&#x7A7A;&#x51FA;&#x4E16;&#xFF0C;&#x5BA3;&#x5E03;&#x5728;ImagenNet&#x56FE;&#x50CF;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x53D6;&#x5F97;&#x4E86;&#x6BD4;&#x4EBA;&#x7C7B;&#x66F4;&#x9AD8;&#x7684;&#x8BC6;&#x522B;&#x80FD;&#x529B;. &#x6B64;&#x7A81;&#x7834;&#x610F;&#x4E49;&#x91CD;&#x5927;&#xFF0C;&#x6587;&#x7AE0;&#x53D1;&#x5E03;&#x540E;&#x5F15;&#x8D77;&#x4E00;&#x7247;&#x70ED;&#x6F6E;&#xFF0C;&#x5728;&#x56FE;&#x50CF;&#x9886;&#x57DF;&#x5177;&#x6709;&#x666E;&#x9002;&#x7684;&#x5E94;&#x7528;.<br>&#x672C;&#x6587;&#x4E2D;&#x7B14;&#x8005;&#x4EC5;&#x5C31;Google&#x7684;Batch Normalization&#x8C08;&#x8C08;&#x7C97;&#x6D45;&#x7684;&#x7406;&#x89E3;.     </p>
<h4 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h4><p>1.1 internal convariate shit<br>&#x4F20;&#x7EDF;&#x7684;&#x6D45;&#x5C42;&#x5B66;&#x4E60;&#x6A21;&#x578B;&#xFF0C;&#x5982;&#x5355;&#x5C42;&#x7684;logistic regression, SVMs&#x4EE5;&#x53CA;2&#x5C42;&#x7ED3;&#x6784;&#x7684;FMs&#x7B49;&#x6A21;&#x578B;&#xFF0C;&#x6BCF;&#x6B21;&#x66F4;&#x65B0;&#x53C2;&#x6570;&#x5747;&#x4ECE;&#x7A33;&#x5B9A;&#x7684;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x4E0A;&#x62DF;&#x5408;. &#x6DF1;&#x5EA6;&#x5B66;&#x4E60;&#x56E0;&#x5176;&#x591A;&#x5C42;&#x7ED3;&#x6784;&#xFF0C;&#x6D45;&#x5C42;&#x8F93;&#x51FA;&#x4F5C;&#x4E3A;&#x4E0B;&#x4E00;&#x5C42;&#x8F93;&#x5165;.&#x9664;&#x4E86;&#x68AF;&#x5EA6;&#x6D88;&#x5931;&#x95EE;&#x9898;&#x5916;&#xFF0C;&#x5728;&#x5B66;&#x4E60;&#x8FC7;&#x7A0B;&#x4E2D;&#xFF0C;&#x6BCF;&#x5C42;&#x7F51;&#x7EDC;&#x7684;&#x53C2;&#x6570;&#x4E0D;&#x65AD;&#x66F4;&#x65B0;&#xFF0C;&#x5BFC;&#x81F4;&#x4E0B;&#x4E00;&#x5C42;&#x8F93;&#x5165;&#x7684;&#x5206;&#x5E03;&#x4E0D;&#x65AD;&#x53D8;&#x5316;. &#x56E0;&#x800C;&#x65E0;&#x6CD5;&#x8DDF;&#x6D45;&#x5C42;&#x6A21;&#x578B;&#x4E00;&#x6837;&#xFF0C;&#x6BCF;&#x6B21;&#x90FD;&#x5728;&#x7A33;&#x5B9A;&#x7684;&#x6570;&#x636E;&#x4E0A;&#x5B66;&#x4E60;&#x53C2;&#x6570;. &#x9664;&#x4E86;&#x964D;&#x4F4E;&#x5B66;&#x4E60;&#x7387;&#x5916;&#xFF0C;&#x8FD8;&#x8981;&#x521D;&#x59CB;&#x5316;&#x4E00;&#x7EC4;&#x826F;&#x597D;&#x7684;&#x53C2;&#x6570;&#xFF0C;&#x8C03;&#x5F97;&#x4E00;&#x624B;&#x597D;&#x53C2;.<br>1.2 &#x4E3A;&#x4EC0;&#x4E48;&#x8981;&#x521D;&#x59CB;&#x5316;&#x4E00;&#x7EC4;&#x826F;&#x597D;&#x7684;&#x53C2;&#x6570;<br>&#x5B66;&#x4E60;&#x8FC7;<a href="http://deeplearning.stanford.edu/tutorial/" target="_blank" rel="external">UFLDL</a>&#x6216;&#x8005;&#x505A;&#x8FC7;&#x56FE;&#x50CF;&#x5B9E;&#x9A8C;&#x7684;&#x540C;&#x5B66;&#x4F1A;&#x53D1;&#x73B0;&#x5BF9;&#x6570;&#x636E;&#x8FDB;&#x884C;&#x9884;&#x5904;&#x7406;&#xFF0C;&#x4F8B;&#x5982;PCA Whitening&#x6216;&#x8005;&#x7B80;&#x5355;&#x7684;z-score&#x90FD;&#x662F;&#x53EF;&#x4EE5;&#x52A0;&#x901F;&#x6536;&#x655B;&#x7684;.<br>&#x9996;&#x5148;&#x56FE;&#x50CF;&#x6570;&#x636E;&#x662F;&#x5C40;&#x90E8;&#x9AD8;&#x5EA6;&#x76F8;&#x5173;&#x7684;&#xFF0C;&#x5E76;&#x4E14;&#x4E2A;&#x7EF4;&#x5EA6;&#x4E0A;&#x7684;&#x6570;&#x636E;&#x53D6;&#x503C;&#x5728;[0,255]&#x4E4B;&#x95F4;. &#x7B80;&#x5316;&#x5230;2&#x7EF4;&#xFF0C;&#x56FE;&#x50CF;&#x6570;&#x636E;&#x4EC5;&#x4F1A;&#x843D;&#x5230;&#x7B2C;&#x4E00;&#x8C61;&#x9650;&#xFF0C;&#x5982;&#x4E0B;&#x56FE;:<br><img src="/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/./1482241848230.png" alt=""><br>&#x56FE;&#x7247;&#x6765;&#x81EA;happynear&#x7684;&#x535A;&#x5BA2;<sup><a href="http://blog.csdn.net/happynear/article/details/44238541" target="_blank" rel="external">2</a></sup><br>&#x5047;&#x8BBE;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x4E3A;ReLu$f(x) = \text{max}(Wx + b)$, &#x5982;&#x679C;&#x4E0D;&#x901A;&#x8FC7;&#x7CBE;&#x5FC3;&#x8BBE;&#x8BA1;&#x6216;&#x8005;fine-tune&#xFF0C;&#x800C;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;$W$, &#x5B66;&#x4E60;&#x7684;&#x524D;&#x671F;&#x9636;&#x6BB5;&#x5F88;&#x53EF;&#x80FD;&#x662F;&#x56FE;&#x4E2D;&#x7684;&#x7EFF;&#x8272;&#x865A;&#x7EBF;&#xFF0C;&#x9700;&#x8981;&#x7ECF;&#x8FC7;&#x957F;&#x65F6;&#x95F4;&#x7684;&#x8FED;&#x4EE3;&#x624D;&#x80FD;&#x6536;&#x655B;&#x5230;&#x7D2B;&#x8272;&#x865A;&#x7EBF;&#xFF0C;&#x5F97;&#x5230;&#x4E00;&#x4E2A;&#x597D;&#x7684;&#x62DF;&#x5408;&#x7ED3;&#x679C;&#x3002;<br>&#x5982;&#x679C;&#x5BF9;&#x6570;&#x636E;&#x505A;&#x9884;&#x5904;&#x7406;&#xFF0C;&#x4F8B;&#x5982;z-score/PCA whitening&#x7B49;&#x7EBF;&#x6027;&#x53D8;&#x5316;&#xFF0C;&#x6620;&#x5C04;&#x5230;0&#x5747;&#x503C;&#x5355;&#x4F4D;&#x65B9;&#x5DEE;&#x7684;&#x5E73;&#x79FB;&#x540E;&#x7A7A;&#x95F4;&#x91CC;&#xFF0C;&#x90A3;&#x4E48;&#x6536;&#x655B;&#x6548;&#x679C;&#x4F1A;&#x6709;&#x663E;&#x8457;&#x63D0;&#x5347;&#x3002;&#x56E0;&#x4E3A;&#x51CF;&#x53BB;&#x5747;&#x503C;&#x540E;&#xFF0C;&#x6570;&#x636E;&#x80FD;&#x5206;&#x6563;&#x5230;&#x5404;&#x4E2A;&#x8C61;&#x9650;&#xFF1B;&#x66F4;&#x8FDB;&#x4E00;&#x6B65;&#x7684;&#xFF0C;&#x5BF9;&#x6570;&#x636E;&#x505A;&#x53BB;&#x76F8;&#x5173;&#x64CD;&#x4F5C;&#xFF0C;&#x63D0;&#x9AD8;&#x6536;&#x655B;&#x6548;&#x7387;.<br>1.3 &#x4E3A;&#x4EC0;&#x4E48;&#x505A;&#x7684;&#x662F;&#x7EBF;&#x6027;&#x53D8;&#x6362;&#x800C;&#x4E0D;&#x80FD;&#x505A;&#x975E;&#x7EBF;&#x6027;&#x53D8;&#x6362;&#x5462;&#xFF1F;<br>Batch Normalization&#x662F;&#x6570;&#x636E;&#x5C42;&#x9762;&#x7684;&#x6539;&#x8FDB;&#xFF0C;&#x8981;&#x4FDD;&#x7559;&#x6570;&#x636E;&#x7684;&#x539F;&#x8C8C;&#xFF0C;&#x5373;&#x4FDD;&#x7559;&#x7279;&#x5F81;&#x7684;&#x975E;&#x7EBF;&#x6027;&#x5173;&#x7CFB;.</p>
<h4 id="Normalization-via-Mini-Batch-Statistics"><a href="#Normalization-via-Mini-Batch-Statistics" class="headerlink" title="Normalization via Mini-Batch Statistics"></a>Normalization via Mini-Batch Statistics</h4><p><img src="/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/./1482241891953.png" alt="&#x7B97;&#x6CD5;&#x793A;&#x610F;&#x56FE;"><br>&#x524D;&#x5411;&#x53CD;&#x9988;&#x65F6;&#xFF0C;&#x5728;&#x6BCF;&#x4E2A;Batch&#x4E2D;&#xFF0C;&#x4ECE;&#x7279;&#x5F81;&#x7EF4;&#x5EA6;&#x4E0A;&#x8BA1;&#x7B97;&#x51FA;mini-batch&#x548C;mini-batch variance&#x4E4B;&#x540E;&#x5B8C;&#x6210;normalization.<br>&#x7406;&#x60F3;&#x60C5;&#x51B5;&#x4E0B;&#xFF0C;&#x5747;&#x503C;&#x548C;&#x65B9;&#x5DEE;&#x5E94;&#x8BE5;&#x662F;&#x5728;&#x6574;&#x4E2A;&#x6570;&#x636E;&#x96C6;&#x4E0A;&#x8BA1;&#x7B97;&#x7684;&#xFF0C;&#x7136;&#x800C;&#x4E0D;&#x80FD;&#x7A7F;&#x8D8A;&#x5F97;&#x5230;unseen  data&#xFF0C;&#x56E0;&#x6B64;&#xFF0C;&#x9000;&#x800C;&#x6C42;&#x5176;&#x6B21;&#xFF0C;&#x7528;Batch&#x4E2D;&#x7EDF;&#x8BA1;&#x7684;&#x5747;&#x503C;&#x548C;&#x65B9;&#x5DEE;&#x4F5C;&#x4E3A;&#x6574;&#x4E2A;&#x6570;&#x636E;&#x96C6;&#x7684;&#x4F30;&#x8BA1;.<br>&#x5B8C;&#x6210;normalization&#x4E4B;&#x540E;&#x7B97;&#x6CD5;&#x4F3C;&#x4E4E;&#x8BE5;&#x7ED3;&#x675F;&#x4E86;&#xFF0C;&#x4F46;&#x662F;&#x5982;&#x679C;&#x628A;&#x7279;&#x5F81;&#x90FD;normalize&#x5230;$\cal{N}(0,1)$&#xFF0C;&#x90A3;&#x4E48;&#x56E0;&#x4E3A;&#x7279;&#x5F81;&#x53EA;&#x5728;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x4E0A;&#x7EBF;&#x6027;&#x533A;&#x57DF;&#x4E0A;&#x6FC0;&#x6D3B;&#xFF0C;&#x4F1A;&#x964D;&#x4F4E;&#x7279;&#x5F81;&#x7684;&#x8868;&#x8FBE;&#x80FD;&#x529B;. &#x5982;&#x4E0B;&#x56FE;&#x865A;&#x7EBF;&#x548C;Sigmoid&#x66F2;&#x7EBF;&#x7684;&#x91CD;&#x53E0;&#x7684;&#x90E8;&#x5206;.<br><img src="/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/./1482241957671.png" alt=""></p>
<p>&#x5BB9;&#x6613;&#x6DF7;&#x6DC6;&#x7684;&#x662F;&#xFF0C;Hinton&#x8001;&#x7237;&#x5B50;&#x66FE;&#x5728;&#x516C;&#x5F00;&#x8BFE;Neural Networks for Machine Learning<sup><a href="https://www.coursera.org/learn/neural-networks" target="_blank" rel="external">3</a></sup>&#x91CC;&#x63D0;&#x5230;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;weight&#x5E94;&#x8BE5;&#x521D;&#x59CB;&#x5316;&#x5728;$\cal{N}(0,1)$&#x9644;&#x8FD1;&#xFF0C;&#x9632;&#x6B62;&#x68AF;&#x5EA6;&#x6D88;&#x5931;.<br><img src="/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/./1482241985728.png" alt=""></p>
<p>&#x4F46;&#x662F;&#xFF0C;&#x5982;&#x679C;&#x4F7F;&#x7528;PReLU&#x6216;&#x8005;ELU&#x7B49;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#xFF0C;&#x662F;&#x4E0D;&#x4F1A;&#x6709;&#x8FD9;&#x4E2A;&#x95EE;&#x9898;&#x7684;!</p>
<p>&#x9488;&#x5BF9;&#x4E0A;&#x8FF0;&#x95EE;&#x9898;&#xFF0C;&#x5728;&#x7B97;&#x6CD5;&#x7ED3;&#x675F;&#x4E4B;&#x524D;&#xFF0C;&#x4F5C;&#x8005;&#x5BF9;normalization&#x4E4B;&#x540E;&#x7684;&#x6570;&#x636E;$\hat{x_i}$&#x8BBE;&#x7F6E;&#x4E86;&#x4E24;&#x4E2A;&#x53C2;&#x6570;$\gamma$&#x548C;$\beta$. &#x5F88;&#x663E;&#x7136;&#x5982;&#x679C;$\gamma=\sqrt{\sigma^2_{\beta}}$,$\beta=\mu_{\beta}$&#xFF0C;&#x90A3;&#x4E48;&#x5BF9;$x_i$ scale<br> and shift&#x4E4B;&#x540E;&#x7684;$y_i$&#x5C31;&#x8FD8;&#x539F;&#x6210;$x_i$&#x4E86;. &#x81F3;&#x4E8E;&#x662F;&#x5426;&#x9700;&#x8981;&#x5BF9;$y_i$&#x8FDB;&#x884C;&#x8FD8;&#x539F;&#xFF0C;&#x7531;&#x6784;&#x5EFA;&#x597D;&#x7684;&#x6A21;&#x578B;&#x81EA;&#x52A8;&#x4ECE;&#x8BAD;&#x7EC3;&#x6570;&#x636E;&#x4E2D;&#x5B66;&#x4E60;&#x51B3;&#x5B9A;&#xFF0C; learning from data.     </p>
<p>&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x65F6;&#xFF0C;SGD&#x4E5F;&#x5728;Batch&#x4E2D;&#x8BA1;&#x7B97;&#xFF0C;&#x68AF;&#x5EA6;&#x516C;&#x5F0F;&#x5F88;&#x7B80;&#x5355;&#xFF0C;&#x8BF7;&#x5927;&#x5BB6;&#x505C;&#x4E0B;10&#x5206;&#x949F;&#xFF0C;&#x5728;&#x767D;&#x7EB8;&#x4E0A;&#x63A8;&#x5BFC;&#x4E00;&#x904D;&#xFF0C;&#x4FDD;&#x8BC1;&#x6E05;&#x695A;&#x7406;&#x89E3;&#x94FE;&#x5F0F;&#x6CD5;&#x5219;&#x8BA1;&#x7B97;&#x68AF;&#x5EA6;&#x7684;&#x601D;&#x8DEF;. &#x8FD9;&#x5F88;&#x91CD;&#x8981;&#xFF0C;&#x56E0;&#x4E3A;&#x76EE;&#x524D;&#x6240;&#x6709;&#x7684;&#x5B66;&#x4E60;&#x7B97;&#x6CD5;&#x90FD;&#x662F;&#x57FA;&#x4E8E;&#x94FE;&#x5F0F;&#x6CD5;&#x5219;&#x7684;&#x53CD;&#x5411;&#x4F20;&#x64AD;.<br><img src="/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/./1482242033059.png" alt="&#x68AF;&#x5EA6;&#x7684;&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x516C;&#x5F0F;"></p>
<h4 id="Implementation-in-Caffe"><a href="#Implementation-in-Caffe" class="headerlink" title="Implementation in Caffe"></a>Implementation in Caffe</h4><p>&#x7B14;&#x8005;&#x63A5;&#x4E0B;&#x6765;&#x4ECB;&#x7ECD;Batch Normalization&#x5728;Caffe&#x4E2D;&#x7684;&#x5B9E;&#x73B0;.   </p>
<p>&#x5B9E;&#x9645;&#x5E94;&#x7528;&#x4E2D;&#xFF0C;$\mu_{\beta}$&#x548C;$\sigma^2_{\beta}$&#x901A;&#x5E38;&#x662F;&#x5728;&#x8BAD;&#x7EC3;&#x96C6;&#x4E0A;&#x8BA1;&#x7B97;&#xFF0C;&#x6D4B;&#x8BD5;&#x7684;&#x65F6;&#x5019;&#x76F4;&#x63A5;&#x4F7F;&#x7528;&#x8BAD;&#x7EC3;&#x65F6;&#x8BA1;&#x7B97;&#x5F97;&#x5230;&#x7684;&#x503C;. &#x6B64;&#x5916;&#xFF0C;Batch Normalization Layer&#x7684;backward pass&#x5B9E;&#x9645;&#x5E76;&#x6CA1;&#x6709;&#x88AB;&#x8C03;&#x7528;&#xFF0C;&#x56E0;&#x6B64;&#x7B14;&#x8005;&#x4EC5;&#x5206;&#x6790;forward pass&#x90E8;&#x5206;&#x7684;&#x4EE3;&#x7801;.    </p>
<p>&#x9996;&#x5148;Batch Normalization&#x5728;proto&#x4E2D;&#x9ED8;&#x8BA4;&#x53C2;&#x6570;&#x914D;&#x7F6E;&#x5982;&#x4E0B;:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">message BatchNormParameter {</div><div class="line">    <span class="comment">// If false, accumulate global mean/variance values via a moving average. If</span></div><div class="line">    <span class="comment">// true, use those accumulated values instead of computing mean/variance</span></div><div class="line">    <span class="comment">// across the batch.</span></div><div class="line">    optional bool use_global_stats = <span class="number">1</span>;</div><div class="line">    <span class="comment">// How much does the moving average decay each iteration?</span></div><div class="line">    optional <span class="type">float</span> moving_average_fraction = <span class="number">2</span> [<span class="section">default</span> = <span class="number">.999</span>];</div><div class="line">    <span class="comment">// Small value to add to the variance estimate so that we don&apos;t divide by</span></div><div class="line">    <span class="comment">// zero.</span></div><div class="line">    optional <span class="type">float</span> eps = <span class="number">3</span> [<span class="section">default</span> = <span class="number">1e-5</span>];</div><div class="line">}</div></pre></td></tr></table></figure></p>
<p>&#x56E0;&#x4E3A;&#x4E00;&#x4E9B;&#x5386;&#x53F2;&#x539F;&#x56E0;(&#x53EF;&#x80FD;&#x662F;scale and shift&#x53EA;&#x5BF9;&#x4F7F;&#x7528;sigmoid&#x4F5C;&#x4E3A;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x6709;&#x6548;), Caffe&#x7684;normalize step &#x548C;scale and shift step&#x81F3;&#x4ECA;&#x4E0D;&#x5728;&#x540C;&#x4E00;&#x4E2A;layer&#x4E2D;&#x5B9E;&#x73B0;, &#x5BFC;&#x81F4;&#x5F88;&#x591A;&#x4EBA;&#x5728;&#x4F7F;&#x7528;&#x7684;&#x65F6;&#x5019;&#x7ECF;&#x5E38;&#x51FA;&#x73B0;&#x4E0D;&#x77E5;&#x9053;&#x8BE5;&#x600E;&#x4E48;&#x7528;&#x6216;&#x8005;&#x8FD9;&#x4E48;&#x7528;&#x5BF9;&#x4E0D;&#x5BF9;&#x7684;&#x95EE;&#x9898;. &#x7B14;&#x8005;&#x5EFA;&#x8BAE;&#x53C2;&#x8003;ResNet<sup><a href="https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-152-deploy.prototxt" target="_blank" rel="external">4</a></sup>&#xFF0C;&#x5E76;&#x5EFA;&#x8BAE;&#x5728;BatchNorm&#x4E2D;&#x4E0D;&#x663E;&#x793A;&#x914D;&#x7F6E;batch_norm_param&#xFF0C;&#x800C;&#x662F;&#x6709;&#x4EE3;&#x7801;&#x8FD0;&#x884C;&#x65F6;&#x81EA;&#x52A8;&#x5224;&#x65AD;&#x662F;&#x5426;use_global_stats&#xFF0C;&#x6B64;&#x5916;&#x4E0D;&#x5EFA;&#x8BAE;&#x505A;scale and shift step&#xFF0C;&#x56E0;&#x4E3A;sigmoid function&#x6548;&#x679C;&#x901A;&#x5E38;&#x662F;&#x5F88;&#x5DEE;&#x7684;&#xFF0C;&#x5BFC;&#x81F4;&#x66F4;&#x591A;&#x7684;&#x662F;&#x4E00;&#x5F00;&#x59CB;&#x5C31;&#x9009;&#x62E9;PReLu&#x7B49;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;:<br><figure class="highlight dts"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="class">layer </span>{</div><div class="line"><span class="symbol">	bottom:</span> <span class="string">&quot;conv1&quot;</span></div><div class="line"><span class="symbol">	top:</span> <span class="string">&quot;conv1&quot;</span></div><div class="line"><span class="symbol">	name:</span> <span class="string">&quot;bn_conv1&quot;</span></div><div class="line"><span class="symbol">	type:</span> <span class="string">&quot;BatchNorm&quot;</span></div><div class="line">	<span class="comment">// &#x4E0D;&#x5EFA;&#x8BAE;&#x914D;&#x7F6E;batch_norm_param. </span></div><div class="line">	<span class="class">batch_norm_param </span>{</div><div class="line"><span class="symbol">		use_global_stats:</span> true</div><div class="line">	}</div><div class="line">} </div><div class="line"></div><div class="line"><span class="class">layer </span>{</div><div class="line"><span class="symbol">	bottom:</span> <span class="string">&quot;conv1&quot;</span></div><div class="line"><span class="symbol">	top:</span> <span class="string">&quot;conv1&quot;</span></div><div class="line"><span class="symbol">	name:</span> <span class="string">&quot;scale_conv1&quot;</span></div><div class="line"><span class="symbol">	type:</span> <span class="string">&quot;Scale&quot;</span></div><div class="line">	<span class="class">scale_param </span>{</div><div class="line"><span class="symbol">		bias_term:</span> true</div><div class="line">	}</div><div class="line">}</div></pre></td></tr></table></figure></p>
<p>&#x8BE6;&#x7EC6;&#x4EE3;&#x7801;&#x8D70;&#x8BFB;&#x53C2;&#x89C1;&#x7B14;&#x8005;&#x7684;<a href="https://github.com/irwenqiang/caffe/blob/master/src/caffe/layers/batch_norm_layer.cpp" target="_blank" rel="external">github</a>.   </p>
<h4 id="&#x603B;&#x7ED3;"><a href="#&#x603B;&#x7ED3;" class="headerlink" title="&#x603B;&#x7ED3;"></a>&#x603B;&#x7ED3;</h4><p>&#x672C;&#x6587;&#x9996;&#x5148;&#x89E3;&#x91CA;&#x4E86;Batch Normalization&#x7684;motivation&#x4EE5;&#x53CA;&#x8BE5;&#x65B9;&#x6CD5;&#x4E3A;&#x4EC0;&#x4E48;work. &#x63A5;&#x7740;&#x5BF9;&#x7B97;&#x6CD5;&#x5185;&#x5BB9;&#x8FDB;&#x884C;&#x8BE6;&#x89E3;&#xFF0C;&#x7ED9;&#x51FA;&#x5E76;&#x5EFA;&#x8BAE;&#x8BFB;&#x8005;&#x670B;&#x53CB;&#x4EEC;&#x52A8;&#x624B;&#x5728;&#x767D;&#x7EB8;&#x4E0A;&#x63A8;&#x5BFC;&#x53CD;&#x5411;&#x4F20;&#x64AD;&#x8BA1;&#x7B97;$\mu_{\beta}$&#x548C;$\sigma^2_{\beta}$&#x7684;&#x68AF;&#x5EA6;&#x516C;&#x5F0F;. &#x6700;&#x540E;&#x5728;github&#x4E0A;&#x63D0;&#x4F9B;&#x4E86;&#x4E00;&#x4EFD;Caffe&#x7684;bn&#x4EE3;&#x7801;&#x8D70;&#x8BFB;.       </p>
<p>&#x6B22;&#x8FCE;&#x5E76;&#x9F13;&#x52B1;&#x5BF9;&#x672C;&#x6587;&#x548C;github&#x4E0A;&#x4EE3;&#x7801;&#x8D70;&#x8BFB;&#x6709;&#x4E0D;&#x540C;&#x89C1;&#x89E3;&#x7684;&#x670B;&#x53CB;&#x4EEC;&#x7559;&#x8A00;&#x6216;&#x8005;&#x8054;&#x7CFB;&#x672C;&#x7AD9;&#x7AD9;&#x957F;! </p>
<h4 id="&#x53C2;&#x8003;&#x6587;&#x732E;"><a href="#&#x53C2;&#x8003;&#x6587;&#x732E;" class="headerlink" title="&#x53C2;&#x8003;&#x6587;&#x732E;"></a>&#x53C2;&#x8003;&#x6587;&#x732E;</h4><ol>
<li><a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="external">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>     </li>
<li><a href="http://blog.csdn.net/happynear/article/details/44238541" target="_blank" rel="external">&#x300A;Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift&#x300B;&#x9605;&#x8BFB;&#x7B14;&#x8BB0;&#x4E0E;&#x5B9E;&#x73B0;</a>   </li>
<li><a href="https://www.coursera.org/learn/neural-networks" target="_blank" rel="external">Neural Networks for Machine Learning</a>  </li>
<li><a href="https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-152-deploy.prototxt" target="_blank" rel="external">ResNet-152-deploy</a>   </li>
</ol>
<p>&#x4F5C;&#x8005;&#x5FAE;&#x535A;&#xFF1A;<a href="http://weibo.com/605506205?is_all=1" target="_blank" rel="external">@ryyen</a></p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="https://mlnote.com/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/" data-id="ciyxmubt10000lolp57vzxdcf" class="article-share-link">分享到</a><div class="tags"><a href="/tags/Batch-Normalization/">Batch Normalization</a><a href="/tags/Caffe/">Caffe</a><a href="/tags/neural-networks/">neural-networks</a></div><div class="post-nav"><a href="/2016/12/18/Reading-Notes-of-Word-Embedding/" class="next">Word Embedding札记</a></div><div data-thread-key="2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/" data-title="浅谈Batch Normalization及其Caffe实现" data-url="https://mlnote.com/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/" data-title="浅谈Batch Normalization及其Caffe实现" data-url="https://mlnote.com/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="https://mlnote.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/其他/">其他</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/gbdt/" style="font-size: 15px;">gbdt</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/neural-networks/" style="font-size: 15px;">neural-networks</a> <a href="/tags/neural-networks/" style="font-size: 15px;">neural networks</a> <a href="/tags/word-embedding/" style="font-size: 15px;">word embedding</a> <a href="/tags/learning-to-rank/" style="font-size: 15px;">learning to rank</a> <a href="/tags/query-rewriting/" style="font-size: 15px;">query rewriting</a> <a href="/tags/semantic-matching/" style="font-size: 15px;">semantic matching</a> <a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/SyntaxNet/" style="font-size: 15px;">SyntaxNet</a> <a href="/tags/CRF/" style="font-size: 15px;">CRF</a> <a href="/tags/user-targeting/" style="font-size: 15px;">user targeting</a> <a href="/tags/advertisement/" style="font-size: 15px;">advertisement</a> <a href="/tags/RBMs/" style="font-size: 15px;">RBMs</a> <a href="/tags/fastBDT/" style="font-size: 15px;">fastBDT</a> <a href="/tags/Caffe/" style="font-size: 15px;">Caffe</a> <a href="/tags/lightGBM/" style="font-size: 15px;">lightGBM</a> <a href="/tags/gradient-boosting-framework/" style="font-size: 15px;">gradient boosting framework</a> <a href="/tags/huber/" style="font-size: 15px;">huber</a> <a href="/tags/LST/" style="font-size: 15px;">LST</a> <a href="/tags/LAD/" style="font-size: 15px;">LAD</a> <a href="/tags/classify/" style="font-size: 15px;">classify</a> <a href="/tags/xgboost/" style="font-size: 15px;">xgboost</a> <a href="/tags/code-mannar/" style="font-size: 15px;">code mannar</a> <a href="/tags/gbRank/" style="font-size: 15px;">gbRank</a> <a href="/tags/logisticRank/" style="font-size: 15px;">logisticRank</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/mathjex-公式/" style="font-size: 15px;">mathjex 公式</a> <a href="/tags/model/" style="font-size: 15px;">model</a> <a href="/tags/bayes/" style="font-size: 15px;">bayes</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/">浅谈Batch Normalization及其Caffe实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/18/Reading-Notes-of-Word-Embedding/">Word Embedding札记</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/10/SyntaxNet Neural-Models-of-Syntax/">简介语法分析开源神经网络SyntaxNet</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/18/The-realization-of-GBDT-in-LightGBM-and-FastDBT/">简述FastDBT和LightGBM中GBDT的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/29/xgboost-code-review-with-paper/">XGboost核心源码阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/05/a-guide-to-xgboost-A-Scalable-Tree-Boosting-System/">XGboost: A Scalable Tree Boosting System论文及源码导读</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/02/gradient-boosting-decision-tree-2/">Gradient Boosting Decision Tree[下篇]</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/24/gradient-boosting-decision-tree-1/">Gradient Boosting Decision Tree[上篇]</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/18/gbRank-logsitRank-from-up-to-bottom/">gbRank & logsitcRank自顶向下</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/13/Ranking-Relevance-in-Yahoo-Search/">[笔记]Ranking Relevance in Yahoo Search</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">水滴石穿.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'qiugen'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?ceebf89bb3d2a3b32aff294e42be4ed7";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>