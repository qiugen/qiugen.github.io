<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="enjoy"><title>Word Embedding札记 | 水滴石穿</title><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/4.2.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.0/grids-responsive-min.css"><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.0.0/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="alternate" type="application/atom+xml" href="/atom.xml"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Word Embedding札记</h1><a id="logo" href="/.">水滴石穿</a><p class="description">探索，保持渴望，无所畏惧</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Word Embedding札记</h1><div class="post-meta">Dec 18, 2016<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><a data-thread-key="2016/12/18/Reading-Notes-of-Word-Embedding/" href="/2016/12/18/Reading-Notes-of-Word-Embedding/#comments" class="ds-thread-count"></a><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-4"><a class="toc-link" href="#基于聚类的分布表示-clusteringbased-word-representation"><span class="toc-number">1.</span> <span class="toc-text">基于聚类的分布表示(clusteringbased word representation)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#基于矩阵的分布表示-distributional-representation"><span class="toc-number">2.</span> <span class="toc-text">基于矩阵的分布表示(distributional representation)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#基于神经网络的分布表示-distributed-representation"><span class="toc-number">3.</span> <span class="toc-text">基于神经网络的分布表示(distributed representation)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#word2vec源码及扩展"><span class="toc-number">4.</span> <span class="toc-text">word2vec源码及扩展</span></a></li></ol></div></div><div class="post-content"><p>&#x5982;&#x4F55;&#x5229;&#x7528;&#x6587;&#x672C;&#x7684;&#x4E0A;&#x4E0B;&#x6587;&#x4FE1;&#x606F;&#xFF0C;&#x5F97;&#x5230;&#x66F4;&#x6709;&#x610F;&#x4E49;&#x7684;&#x5411;&#x91CF;&#x8868;&#x8FBE;(word embedding)&#xFF0C;&#x662F;NLP&#x9886;&#x57DF;&#x7814;&#x7A76;&#x7684;&#x91CD;&#x70B9;&#x3002;&#x672C;&#x7BC7;&#x7B14;&#x8BB0;&#x76EE;&#x7684;&#x5728;&#x4E8E;&#x6574;&#x7406;&#x8BCD;&#x5411;&#x91CF;&#x7684;&#x53D1;&#x5C55;&#x5386;&#x7A0B;&#xFF0C;&#x65B9;&#x4FBF;&#x7406;&#x89E3;&#x4EC0;&#x4E48;&#x662F;&#x8BCD;&#x5411;&#x91CF;&#xFF0C;&#x600E;&#x4E48;&#x5F97;&#x5230;&#x8BCD;&#x5411;&#x91CF;&#x3002;&#x8BCD;&#x5411;&#x91CF;&#x4E5F;&#x53EB;&#x8BCD;&#x7684;&#x5206;&#x5E03;&#x5F0F;&#x8868;&#x8FBE;&#xFF0C;&#x4E3B;&#x8981;&#x6709;&#x4E09;&#x7C7B;&#x65B9;&#x6CD5;&#xFF1A;&#x805A;&#x7C7B;&#xFF0C;&#x77E9;&#x9635;&#x5206;&#x89E3;&#xFF0C;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x3002;</p>
<h4 id="&#x57FA;&#x4E8E;&#x805A;&#x7C7B;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;-clusteringbased-word-representation"><a href="#&#x57FA;&#x4E8E;&#x805A;&#x7C7B;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;-clusteringbased-word-representation" class="headerlink" title="&#x57FA;&#x4E8E;&#x805A;&#x7C7B;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;(clusteringbased word representation)"></a>&#x57FA;&#x4E8E;&#x805A;&#x7C7B;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;(clusteringbased word representation)</h4><p>&#x8FD9;&#x7C7B;&#x65B9;&#x6CD5;&#x901A;&#x8FC7;&#x805A;&#x7C7B;&#x5C06;&#x8BCD;&#x548C;&#x7C7B;&#x7684;&#x6807;&#x7B7E;&#x5EFA;&#x7ACB;&#x5173;&#x8054;&#xFF0C;&#x5173;&#x8054;&#x5173;&#x7CFB;&#x53EF;&#x4EE5;&#x662F;&#x786E;&#x5B9A;&#x6027;&#x4E5F;&#x53EF;&#x4EE5;&#x662F;&#x6982;&#x7387;&#x8868;&#x793A;&#xFF0C;&#x7528;&#x8FD9;&#x79CD;&#x65B9;&#x5F0F;&#x6784;&#x5EFA;&#x8BCD;&#x4E0E;&#x5176;&#x4E0A;&#x4E0B;&#x6587;&#x4E4B;&#x95F4;&#x7684;&#x5173;&#x7CFB;&#x3002;&#x76F8;&#x5173;&#x5DE5;&#x4F5C;&#x6709;(Brown, 1992)[10], (Ney, 1993)[35], (Niesler, 1998)[36]&#x3002;&#x5E03;&#x6717;&#x805A;&#x7C7B;(Brown, 1992)[10]&#x662F;&#x4E00;&#x79CD;&#x5C42;&#x7EA7;&#x805A;&#x7C7B;&#x65B9;&#x6CD5;&#xFF0C;&#x805A;&#x7C7B;&#x7ED3;&#x679C;&#x4E3A;&#x6BCF;&#x4E2A;&#x8BCD;&#x7684;&#x591A;&#x5C42;&#x7C7B;&#x522B;&#x4F53;&#x7CFB;&#x3002;&#x53EF;&#x4EE5;&#x6839;&#x636E;&#x4E24;&#x4E2A;&#x8BCD;&#x7684;&#x516C;&#x5171;&#x7C7B;&#x522B;&#x5224;&#x65AD;&#x8FD9;&#x4E24;&#x4E2A;&#x8BCD;&#x7684;&#x8BED;&#x4E49;&#x76F8;&#x4F3C;&#x5EA6;&#x3002;</p>
<h4 id="&#x57FA;&#x4E8E;&#x77E9;&#x9635;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;-distributional-representation"><a href="#&#x57FA;&#x4E8E;&#x77E9;&#x9635;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;-distributional-representation" class="headerlink" title="&#x57FA;&#x4E8E;&#x77E9;&#x9635;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;(distributional representation)"></a>&#x57FA;&#x4E8E;&#x77E9;&#x9635;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;(distributional representation)</h4><p>&#x6784;&#x5EFA;&#x4E00;&#x4E2A;&#x201C;&#x8BCD;-&#x4E0A;&#x4E0B;&#x6587;&#x201D;&#x77E9;&#x9635;&#xFF0C;&#x4ECE;&#x77E9;&#x9635;&#x4E2D;&#x83B7;&#x53D6;&#x8BCD;&#x7684;&#x8868;&#x793A;&#x3002;&#x5728;&#x201C;&#x8BCD;-&#x4E0A;&#x4E0B;&#x6587;&#x201D;&#x77E9;&#x9635;&#x4E2D;&#xFF0C;&#x6BCF;&#x884C;&#x5BF9;&#x5E94;&#x4E00;&#x4E2A;&#x8BCD;&#xFF0C;&#x6BCF;&#x5217;&#x8868;&#x793A;&#x4E00;&#x79CD;&#x4E0D;&#x540C;&#x7684;&#x4E0A;&#x4E0B;&#x6587;&#xFF0C;&#x77E9;&#x9635;&#x4E2D;&#x7684;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x5BF9;&#x5E94;&#x76F8;&#x5173;&#x8BCD;&#x548C;&#x4E0A;&#x4E0B;&#x6587;&#x7684;&#x5171;&#x73B0;&#x6B21;&#x6570;&#x3002;<br> a. &#x77E9;&#x9635;&#x6784;&#x9020;<br> b. &#x77E9;&#x9635;&#x5143;&#x7D20;&#x503C;&#x7684;&#x786E;&#x5B9A;<br> c. &#x964D;&#x7EF4;&#x6280;&#x672F;&#x5C06;&#x9AD8;&#x7EF4;&#x7A00;&#x758F;&#x7684;&#x5411;&#x91CF;&#x538B;&#x7F29;&#x6210;&#x4F4E;&#x7EF4;&#x7A20;&#x5BC6;<br>&#x5178;&#x578B;&#x5982;Latent Semantic Analysis (LSA) &#x7684;&#x505A;&#x6CD5;&#xFF0C;&#x6784;&#x9020;word-doc&#x77E9;&#x9635;&#xFF0C;TF-IDF&#x4E3A;&#x6BCF;&#x4E2A;&#x5143;&#x7D20;&#x7684;&#x503C;&#x3002;&#x4F7F;&#x7528;SVD&#x5206;&#x89E3;&#xFF0C;&#x5F97;&#x5230;&#x8BCD;&#x7684;&#x4F4E;&#x7EF4;&#x8868;&#x8FBE;(Deerwester, 1990)[37] (Bellegarda, 1997)[34]&#x3002;&#x4ECB;&#x7ECD;&#x4E24;&#x4EFD;&#x6BD4;&#x8F83;&#x8FD1;&#x7684;&#x5DE5;&#x4F5C;&#xFF1A;</p>
<ul>
<li>GloVe(Pennington, 2014)[27]&#xFF0C;GloVe &#x6A21;&#x578B;&#x662F;&#x4E00;&#x79CD;&#x5BF9;&#x201C;&#x8BCD;-&#x8BCD;&#x201D;&#x77E9;&#x9635;&#x8FDB;&#x884C;&#x5206;&#x89E3;&#x4ECE;&#x800C;&#x5F97;&#x5230;&#x8BCD;&#x8868;&#x793A;&#x7684;&#x65B9;&#x6CD5;&#x3002;&#x77E9;&#x9635;&#x7B2C;$i$&#x884C;&#x7B2C;$j$&#x5217;&#x7684;&#x503C;&#x4E3A;&#x8BCD;$v_i$&#x4E0E;&#x8BCD;$v_j$ &#x5728;&#x8BED;&#x6599;&#x4E2D;&#x7684;&#x5171;&#x73B0;&#x6B21;&#x6570;$x_{ij}$ &#x7684;&#x5BF9;&#x6570;&#x3002;&#x5728;&#x77E9;&#x9635;&#x5206;&#x89E3;&#x6B65;&#x9AA4;&#xFF0C;GloVe &#x6A21;&#x578B;&#x501F;&#x9274;&#x4E86;LSA (Deerwester,1990)[31]&#xFF0C;&#x5728;&#x8BA1;&#x7B97;&#x91CD;&#x6784;&#x8BEF;&#x5DEE;&#x65F6;&#xFF0C;&#x53EA;&#x8003;&#x8651;&#x5171;&#x73B0;&#x6B21;&#x6570;&#x975E;&#x96F6;&#x7684;&#x77E9;&#x9635;&#x5143;&#x7D20;&#xFF0C;&#x540C;&#x65F6;&#x5BF9;&#x77E9;&#x9635;&#x4E2D;&#x7684;&#x884C;&#x548C;&#x5217;&#x52A0;&#x5165;&#x4E86;&#x504F;&#x79FB;&#x9879;&#xFF0C;&#x6839;&#x636E;&#x5171;&#x73B0;&#x8BCD;&#x9891;&#x5BF9;&#x91CD;&#x6784;&#x8BEF;&#x5DEE;&#x8FDB;&#x884C;&#x8C03;&#x6743;&#x3002;&#x6587;&#x7AE0;&#x8BBA;&#x8FF0;&#x4E86;&#x57FA;&#x4E8E;&#x77E9;&#x9635;&#x5206;&#x89E3;&#x65B9;&#x6CD5;&#x548C;skip-gram/ivLBL&#x578B;&#x4F18;&#x5316;&#x76EE;&#x6807;&#x76F8;&#x4F3C;&#xFF0C;&#x4EE3;&#x7801;&#x5F00;&#x6E90;&#x5728;&#xFF1A;<a href="http://nlp.%20stanford.edu/projects/glove/" target="_blank" rel="external">github</a>&#xFF0C;&#x4E3B;&#x9875;&#x94FE;&#x63A5;&#xFF1A;<a href="http://nlp.stanford.edu/projects/glove/" target="_blank" rel="external">paper</a>&#x3002;&#x5173;&#x4E8E;&#x77E9;&#x9635;&#x5206;&#x89E3;&#x548C;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x65B9;&#x6CD5;&#x4E4B;&#x95F4;&#x5173;&#x7CFB;&#x7684;&#x66F4;&#x591A;&#x8BA8;&#x8BBA;&#x53EF;&#x4EE5;&#x53C2;&#x770B;(Omer Levy, 2014)[25], (Li Y, 2015)[26]</li>
<li>FOREST(Yogatama, 2014)[32]&#xFF0C; FOREST&#x6A21;&#x578B;&#x548C;GloVe&#x4E0D;&#x540C;&#xFF0C;&#x201D;&#x8BCD;-&#x8BCD;&#x201D;&#x77E9;&#x9635;&#x7B2C;$i$&#x884C;&#x7B2C;$j$&#x5217;&#x7684;&#x5143;&#x7D20;&#x4E3A;&#x8BCD;$v_i$&#x4E0E;&#x8BCD;$v_j$&#x7684;&#x4E92;&#x4FE1;&#x606F;&#xFF0C;&#x5C06;&#x4E92;&#x4FE1;&#x606F;&#x77E9;&#x9635;&#x5206;&#x89E3;&#x4E3A;&#x57FA;&#x77E9;&#x9635;&#x548C;&#x8BCD;&#x7684;&#x8868;&#x793A;&#x77E9;&#x9635;&#x7684;&#x4E58;&#x79EF;&#xFF0C;&#x5728;&#x76EE;&#x6807;&#x51FD;&#x6570;&#x4E2D;&#xFF0C;&#x5BF9;&#x8868;&#x793A;&#x77E9;&#x9635;&#x7684;&#x6BCF;&#x4E00;&#x4E2A;&#x7EF4;&#x5EA6;&#xFF0C;&#x5F15;&#x5165;&#x7ED3;&#x6784;&#x5316;&#x6B63;&#x5219;&#x5316;&#x9879;&#x3002;&#x4F7F;&#x7528;&#x5728;&#x7EBF;&#x8BCD;&#x5178;&#x5B66;&#x4E60;&#x65B9;&#x6CD5;&#x8FDB;&#x884C;&#x6C42;&#x89E3;&#x3002;</li>
</ul>
<h4 id="&#x57FA;&#x4E8E;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;-distributed-representation"><a href="#&#x57FA;&#x4E8E;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;-distributed-representation" class="headerlink" title="&#x57FA;&#x4E8E;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;(distributed representation)"></a>&#x57FA;&#x4E8E;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x5206;&#x5E03;&#x8868;&#x793A;(distributed representation)</h4><p>&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x8BCD;&#x5411;&#x91CF;&#x8868;&#x793A;&#x6280;&#x672F;&#x901A;&#x8FC7;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x6280;&#x672F;&#x5BF9;&#x4E0A;&#x4E0B;&#x6587;&#xFF0C;&#x4EE5;&#x53CA;&#x4E0A;&#x4E0B;&#x6587;&#x4E0E;&#x76EE;&#x6807;&#x8BCD;&#x4E4B;&#x95F4;&#x7684;&#x5173;&#x7CFB;&#x8FDB;&#x884C;&#x5EFA;&#x6A21;&#xFF0C;&#x8FD9;&#x7C7B;&#x65B9;&#x6CD5;&#x7684;&#x6700;&#x5927;&#x4F18;&#x52BF;&#x5728;&#x4E8E;&#x53EF;&#x4EE5;&#x8868;&#x793A;&#x590D;&#x6742;&#x7684;&#x4E0A;&#x4E0B;&#x6587;&#x3002;<br>&#x53D1;&#x5C55;&#x8F68;&#x8FF9;&#xFF1A; </p>
<ul>
<li>&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x6C42;&#x89E3;&#x4E8C;&#x5143;&#x8BED;&#x8A00;&#x6A21;&#x578B; (&#x5F90;&#x4F1F;,2000)[13]</li>
<li>Neural Network Language Model (Yoshua Bengio,  2001, 2003)[14,15] <ul>
<li>Hierarchical NNLM(Bengio, 2005)[39] </li>
</ul>
</li>
<li>Log-Bilinear Language Model (Mnih &amp; Hinton, 2007)[16]<ul>
<li>Hierarchical LBL(Mnih, 2008)[17]    {&#x6CE8;&#xFF1A;Hierarchical Softmax}</li>
<li>vector LBL / inverse vector LBL(Mnih, 2013)[18]   {&#x6CE8;&#xFF1A;Skip-gram, &#x566A;&#x58F0;&#x6BD4;&#x4F30;&#x7B97;NCE}</li>
</ul>
</li>
<li>C &amp; W model(Collobert &amp; Weston, 2008)[21]   {&#x6CE8;&#xFF1A;pair-wise negative sampling}</li>
<li>CBOW &amp; Skip-gram model(Mikolov,2013)[22]&#xFF0C; (Mikolov, 2013)[23]</li>
</ul>
<p>&#x63A5;&#x4E0B;&#x6765;&#x6309;&#x65F6;&#x5E8F;&#x4ECB;&#x7ECD;&#x4E0B;&#x4E0A;&#x9762;&#x51E0;&#x7BC7;&#x6587;&#x7AE0;&#xFF1A;</p>
<ol>
<li>(&#x5F90;&#x4F1F;, 2000)[13]&#x63D0;&#x51FA;&#x4F7F;&#x7528;&#x5355;&#x5C42;&#x5168;&#x8054;&#x901A;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x5904;&#x7406;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#xFF0C;&#x5BF9;&#x8BCD;&#x7684;bigram&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x8FDB;&#x884C;&#x5EFA;&#x6A21;&#xFF0C;&#x6700;&#x5C0F;&#x5316;&#x6DF7;&#x6DC6;&#x5EA6;&#x7684;log&#x4F3C;&#x7136;&#x4E3A;&#x76EE;&#x6807;&#xFF0C;&#x8F93;&#x5165;&#x4E3A;&#x6BCF;&#x4E2A;&#x8BCD;&#x7684;one-hot&#x8868;&#x793A;&#xFF0C;&#x8F93;&#x51FA;&#x4E3A;&#x7ED9;&#x5B9A;&#x5F53;&#x524D;&#x8BCD;&#x5176;&#x4ED6;&#x8BCD;&#x7684;&#x6982;&#x7387;&#xFF0C;&#x901A;&#x8FC7;softmax&#x8FDB;&#x884C;&#x5F52;&#x4E00;&#x5316;&#x8F93;&#x51FA;&#x503C;&#x3002;</li>
<li>(Bengio, 2001, 2003][14,15]&#x63D0;&#x51FA;&#x540C;&#x65F6;&#x8BAD;&#x7EC3;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x548C;&#x8BCD;&#x5411;&#x91CF;&#x7684;&#x65B9;&#x6CD5;&#x3002;&#x7B80;&#x4ECB;&#x4E0B;[15, Bengio, 2003]&#x7684;&#x5DE5;&#x4F5C;&#xFF0C;&#x5C06;&#x8BCD;&#x8868;&#x793A;&#x4E3A;m&#x7EF4;&#x7684;&#x7279;&#x5F81;&#x5411;&#x91CF;&#xFF0C;&#x5C06;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x8868;&#x793A;&#x4E3A;&#x8BCD;&#x5411;&#x91CF;&#x7684;&#x6982;&#x7387;&#x51FD;&#x6570;&#xFF0C;&#x540C;&#x65F6;&#x5B66;&#x4E60;&#x8BCD;&#x7684;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x548C;&#x6982;&#x7387;&#x51FD;&#x6570;&#x7684;&#x53C2;&#x6570;&#x3002;&#x7F51;&#x7EDC;&#x7ED3;&#x6784;&#x5982;&#x4E0B;&#x56FE;&#xFF1A;<br><img src="/2016/12/18/Reading-Notes-of-Word-Embedding/./1481042151022.png" alt=""><br>&#x7EFF;&#x8272;&#x865A;&#x7EBF;&#x8868;&#x793A;&#x8F93;&#x5165;x&#x6709;&#x76F4;&#x8FDE;&#x5230;softmax&#x5C42;&#xFF0C;Bengio&#x6A21;&#x578B;&#x4E2D;&#x8FD9;&#x90E8;&#x5206;&#x662F;&#x53EF;&#x9009;&#x7684;&#xFF0C;&#x56E0;&#x6B64;&#x4E5F;&#x6709;&#x5176;&#x4ED6;&#x5730;&#x65B9;&#x7684;&#x56FE;&#x662F;&#x7565;&#x6389;&#x8FD9;&#x6761;&#x865A;&#x7EBF;&#x3002;&#x76F4;&#x8FDE;&#x8FB9;&#x80FD;&#x8BA9;&#x6A21;&#x578B;&#x5B66;&#x4E60;&#x5230;&#x8BCD;&#x7684;&#x6743;&#x91CD;&#x66F4;&#x591A;&#xFF0C;&#x52A0;&#x5FEB;&#x6536;&#x655B;&#x901F;&#x5EA6;&#x3002;&#x5982;&#x679C;&#x53BB;&#x9664;&#x76F4;&#x8FDE;&#x7684;&#x8BDD;&#xFF0C;&#x901A;&#x8FC7;&#x9690;&#x5C42;&#x80FD;&#x5B66;&#x4E60;&#x5230;&#x8F83;&#x591A;&#x7684;&#x6CDB;&#x5316;&#x3002;&#x7531;&#x4E8E;&#x6FC0;&#x6D3B;&#x51FD;&#x6570;&#x548C;&#x5F52;&#x4E00;&#x5316;&#x90E8;&#x5206;&#x8BA1;&#x7B97;&#x8017;&#x65F6;&#xFF0C;&#x8BBA;&#x6587;&#x91CC;&#x8BA8;&#x8BBA;&#x4E86;&#x4E24;&#x79CD;&#x5EFA;&#x7ACB;&#x5728;&#x5F02;&#x6B65;&#x66F4;&#x65B0;&#x6743;&#x91CD;&#x7684;&#x5E76;&#x884C;&#x65B9;&#x6CD5;&#xFF1A;&#x5171;&#x4EAB;&#x5185;&#x5B58;&#x7684;&#x591A;&#x5904;&#x7406;&#x5668;&#x5E76;&#x884C;&#x548C;&#x7F51;&#x7EDC;&#x5E76;&#x884C;&#x3002;</li>
<li>(Bengio, 2005)[39]&#x63D0;&#x51FA;&#x4E86;Hierarchical NNLM&#xFF0C;&#x901A;&#x8FC7;&#x8BCD;&#x4E49;&#x4E4B;&#x95F4;&#x7684;&#x5173;&#x7CFB;&#x542F;&#x53D1;&#x5F0F;&#x7684;&#x6784;&#x5EFA;&#x4E8C;&#x53C9;&#x6811;&#xFF0C;&#x4EE3;&#x66FF;&#x5F52;&#x4E00;&#x5316;&#x8BA1;&#x7B97;&#xFF0C;&#x5C06;&#x8BAD;&#x7EC3;&#x901F;&#x5EA6;&#x63D0;&#x5347;200&#x500D;&#x3002;&#x6784;&#x5EFA;&#x6811;&#x7684;&#x542F;&#x53D1;&#x6765;&#x81EA;(Goodman, 2001)[40]&#xFF0C;&#x5C06;&#x8BA1;&#x7B97;&#x6761;&#x4EF6;&#x6982;&#x7387;&#x8F6C;&#x4E3A;&#x4E3A;&#x4E00;&#x7CFB;&#x5217;&#x5B50;&#x5206;&#x7C7B;&#x6982;&#x7387;&#x7684;&#x4E58;&#x79EF;&#xFF0C;&#x5BF9;&#x4E8E;&#x5B50;&#x5206;&#x7C7B;&#x95EE;&#x9898;&#xFF0C;&#x6BCF;&#x4E00;&#x4E2A;&#x5206;&#x652F;&#xFF0C;Bengio&#x4F7F;&#x7528;&#x540C;&#x4E00;&#x4E2A;&#x5206;&#x7C7B;&#x5668;&#x8FDB;&#x884C;&#x51B3;&#x7B56;&#xFF0C;&#x7531;&#x4E8E;&#x5206;&#x7C7B;&#x7684;&#x9700;&#x6C42;&#x4F7F;&#x5F97;&#x6BCF;&#x4E00;&#x4E2A;&#x53F6;&#x5B50;&#x8282;&#x70B9;&#x9700;&#x8981;&#x5177;&#x5907;&#x7C7B;&#x522B;&#x610F;&#x4E49;&#xFF0C;&#x5F88;&#x81EA;&#x7136;&#x5730;&#x5F15;&#x5165;&#x4E86;WordNet&#x8BCD;&#x4E4B;&#x95F4;&#x7684;&#x5173;&#x7CFB;&#x542F;&#x53D1;&#x5F0F;&#x5730;&#x6784;&#x5EFA;&#x53F6;&#x5B50;&#x8282;&#x70B9;&#x3002;</li>
<li>(Mnih &amp; Hinton, 2007)[16]&#x7528;RBM&#x5EFA;&#x6A21;LM&#xFF0C;&#x63D0;&#x51FA;&#x4E86;&#x4E09;&#x4E2A;&#x6A21;&#x578B;&#xFF0C;&#x5206;&#x522B;&#x662F;Factored RBM, Temporal Factored RBM&#x548C;Log-Bilinear Language Model&#x3002; &#x8BED;&#x8A00;&#x6A21;&#x578B;&#x7ED3;&#x6784;&#x5982;&#x4E0B;&#x56FE;&#xFF1A;<img src="/2016/12/18/Reading-Notes-of-Word-Embedding/./1481442747480.png" alt=""><br>&#x5176;&#x4E2D;$ v_i $&#x4E3A;&#x8BCD;&#x7684;index&#x5411;&#x91CF;&#xFF0C;$R$&#x4E3A;&#x7279;&#x5F81;&#x5411;&#x91CF;&#x8868;&#xFF0C;&#x4E8C;&#x8005;&#x76F8;&#x4E58;&#x53EF;&#x5F97;&#x5230;&#x7B2C;$i$&#x4E2A;&#x8BCD;&#x7684;&#x5411;&#x91CF;&#x8868;&#x8FBE;&#x3002;$h$&#x4E3A;&#x9690;&#x5C42;&#x3002;a). &#x5DE6;&#x8FB9;&#x6A21;&#x578B;&#x7ED3;&#x6784;&#x5B9E;&#x7EBF;&#x90E8;&#x5206;&#x4E3A;Factored RBM&#xFF0C;&#x7ED3;&#x5408;&#x865A;&#x7EBF;&#x6846;&#x90E8;&#x5206;&#x6784;&#x6210;Temporal Factored RBM&#xFF0C;&#x865A;&#x7EBF;&#x6846;&#x8868;&#x793A;$h_{t-1}$&#xFF0C;b). &#x53F3;&#x8FB9;&#x7ED3;&#x6784;&#x4E3A;Log-Bilinear Language Model&#x3002;&#x80FD;&#x91CF;&#x8868;&#x8FBE;&#x5F0F;&#x4E2D;&#x53BB;&#x9664;&#x4E86;&#x9690;&#x5C42;&#xFF0C;&#x7B80;&#x5316;&#x7F51;&#x7EDC;&#x7ED3;&#x6784;&#xFF0C;&#x63D0;&#x5347;&#x8BAD;&#x7EC3;&#x901F;&#x5EA6;&#x3002;&#x4E09;&#x4E2A;&#x6A21;&#x578B;&#x90FD;&#x662F;&#x901A;&#x8FC7;Constrastiv Divergence&#x7B97;&#x6CD5;(Hinton, 2002)[38]&#x8FDB;&#x884C;&#x8BAD;&#x7EC3;&#x3002;</li>
<li>(Mnih &amp; Hinton, 2008)[17]&#x5728;(Bengio, 2005)[39]&#x7684;&#x7ED3;&#x6784;&#x5316;NNLM&#x542F;&#x53D1;&#x4E0B;&#x5BF9;LBL&#x8FDB;&#x884C;&#x6539;&#x8FDB;&#xFF0C;&#x63D0;&#x51FA;&#x4E86;Hierarchical LBL&#x3002;&#x4E3A;&#x4E86;&#x964D;&#x4F4E;&#x8BA1;&#x7B97;&#x590D;&#x6742;&#x6027;&#xFF0C;&#x9650;&#x5236;&#x5BF9;&#x4E0A;&#x4E0B;&#x6587;Context&#x7684;&#x5173;&#x7CFB;&#x77E9;&#x9635;&#x4E3A;&#x5BF9;&#x89D2;&#x77E9;&#x9635;&#xFF0C;&#x8FD9;&#x79CD;&#x7B80;&#x5316;&#x76F8;&#x5F53;&#x4E8E;&#x76F4;&#x63A5;&#x8FDB;&#x884C;&#x4E24;&#x4E2A;&#x5411;&#x91CF;&#x5185;&#x79EF;&#x3002;&#x6709;&#x522B;&#x4E8E;&#x5F15;&#x5165;WordNet&#x8D44;&#x6E90;&#xFF0C;Mnih&#x63D0;&#x51FA;&#x53E6;&#x4E00;&#x79CD;&#x601D;&#x8DEF;&#x5EFA;&#x7ACB;&#x8BCD;&#x7684;&#x4E8C;&#x53C9;&#x6811;&#x7ED3;&#x6784;&#xFF0C;&#x968F;&#x673A;&#x521D;&#x59CB;&#x5316;&#x4E8C;&#x53C9;&#x6811;&#xFF0C;&#x8BAD;&#x7EC3;&#x5F97;&#x5230;&#x521D;&#x59CB;&#x8BCD;&#x5411;&#x91CF;&#x3002;&#x4ECE;&#x6839;&#x8282;&#x70B9;&#x5F00;&#x59CB;&#x8FED;&#x4EE3;&#x4F7F;&#x7528;&#x9AD8;&#x65AF;&#x6DF7;&#x5408;&#x6A21;&#x578B;(K=2)&#x8FDB;&#x884C;&#x805A;&#x7C7B;&#xFF0C;&#x4E3A;&#x4E0D;&#x540C;&#x7684;&#x8BCD;&#x9009;&#x62E9;&#x4E0D;&#x540C;&#x7684;&#x5206;&#x652F;&#x3002;&#x5B9E;&#x9A8C;&#x6548;&#x679C;&#x4E0A;&#x770B;&#x8FD9;&#x79CD;&#x542F;&#x53D1;&#x5F0F;&#x5EFA;&#x6811;&#x7684;&#x65B9;&#x5F0F;&#x80FD;&#x660E;&#x663E;&#x63D0;&#x9AD8;LM&#x7684;&#x6548;&#x679C;&#x3002;</li>
<li>(Collobert &amp;Weston, 2008)[21]&#x5C1D;&#x8BD5;&#x4F7F;&#x7528;&#x4E00;&#x5957;&#x6846;&#x67B6;&#x5B8C;&#x6210;&#x591A;&#x9879;&#x4EFB;&#x52A1;&#xFF0C;&#x8BAD;&#x7EC3;LM&#x4EFB;&#x52A1;&#x901A;&#x8FC7;&#x968F;&#x673A;&#x66FF;&#x6362;&#x5F53;&#x524D;&#x8BCD;&#x7684;&#x65B9;&#x5F0F;&#x6784;&#x9020;&#x8D1F;&#x4F8B;&#xFF0C;&#x5F97;&#x5230;&#x5224;&#x522B;&#x5F0F;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x540C;&#x65F6;&#x4EA7;&#x51FA;&#x8BCD;&#x5411;&#x91CF;&#xFF0C;&#x5224;&#x522B;&#x5F0F;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x65E0;&#x9700;&#x8FDB;&#x884C;&#x8F93;&#x51FA;&#x6982;&#x7387;&#x5F52;&#x4E00;&#x5316;&#xFF0C;&#x52A0;&#x5FEB;&#x4E86;&#x8BA1;&#x7B97;&#x901F;&#x5EA6;&#xFF0C;&#x5E76;&#x5C1D;&#x8BD5;&#x4E86;&#x5C06;&#x8BED;&#x8A00;&#x6A21;&#x578B;&#x7ED3;&#x5408;&#x5176;&#x4ED6;&#x5E8F;&#x5217;&#x6807;&#x6CE8;&#x4EFB;&#x52A1;(NER, POS, Semantic Role Labeling)&#x540C;&#x65F6;&#x8BAD;&#x7EC3;&#x7684;&#x65B9;&#x5F0F;&#xFF0C;&#x5404;&#x4E2A;&#x4EFB;&#x52A1;&#x5171;&#x4EAB;&#x8BCD;&#x7684;&#x5411;&#x91CF;&#x8868;&#x8FBE;&#x3002;&#x540E;&#x7EED;&#x5728;&#x8FD9;&#x4E2A;&#x57FA;&#x7840;&#x4E0A;&#x63D0;&#x51FA;&#x4E86;SENNA model(Collobert, 2011)[28]&#x3002;&#x4E0B;&#x56FE;&#x4E3A;&#x591A;&#x4EFB;&#x52A1;&#x8BAD;&#x7EC3;&#x793A;&#x610F;&#x56FE;&#x3002;<br><img src="/2016/12/18/Reading-Notes-of-Word-Embedding/./1481966848316.png" alt=""></li>
<li>(Mikolov, 2013)[22]&#x63D0;&#x51FA;&#x4E86;CBOW(Continuous Bag Of words)&#x548C;skipgram&#x6A21;&#x578B;&#xFF0C;CBOW&#x4F7F;&#x7528;&#x7A97;&#x53E3;&#x4E0A;&#x4E0B;&#x6587;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#xFF0C;&#x9884;&#x6D4B;&#x5F53;&#x524D;&#x8BCD;&#x3002;skipgram&#x4F7F;&#x7528;&#x5F53;&#x524D;&#x8BCD;&#x4F5C;&#x4E3A;&#x8F93;&#x5165;&#xFF0C;&#x9884;&#x6D4B;&#x7A97;&#x53E3;&#x8303;&#x56F4;&#x5185;&#x7684;&#x5176;&#x4ED6;&#x8BCD;&#xFF0C;&#x7CBE;&#x7B80;&#x4E86;&#x9690;&#x5C42;&#xFF0C;&#x8BA1;&#x7B97;&#x6548;&#x7387;&#x5927;&#x4E3A;&#x63D0;&#x9AD8;&#x3002;&#x4E0B;&#x56FE;&#x4E3A;CBOW&#x548C;skipgram&#x7F51;&#x7EDC;&#x7ED3;&#x6784;&#x56FE;&#xFF1A;<br><img src="/2016/12/18/Reading-Notes-of-Word-Embedding/./1482070797585.png" alt=""></li>
<li>(Mnih &amp; Kavukcuoglu, 2013)[18] &#x53D7;Mikolov&#x542F;&#x53D1;&#xFF0C;&#x5728;log-Bilinear Language Model&#x57FA;&#x7840;&#x4E0A;&#x5C06;&#x77E9;&#x9635;&#x8FD0;&#x7B97;&#x7B80;&#x5316;&#x4E3A;&#x5411;&#x91CF;&#x8FD0;&#x7B97;&#x5F97;&#x5230;vector log-Bilinear Model(vLBL)&#xFF0C;&#x8FDB;&#x4E00;&#x6B65;&#x7B80;&#x5316;&#x4E0A;&#x4E0B;&#x6587;&#x7684;&#x8868;&#x8FBE;&#x4E3A;&#x7B80;&#x5355;&#x6C42;&#x548C;&#x7B49;&#x4EF7;&#x4E8E;CBOW&#xFF0C;&#x501F;&#x9274;skipgram&#x7684;&#x601D;&#x60F3;&#xFF1A;&#x4F7F;&#x7528;&#x5F53;&#x524D;&#x8BCD;&#x9884;&#x6D4B;&#x4E0A;&#x4E0B;&#x6587;&#xFF0C;&#x5F97;&#x5230;inverse vector log-Bilinear Model(ivLBL)&#xFF0C;&#x76EE;&#x6807;&#x51FD;&#x6570;&#x91C7;&#x7528;Noise-contrastive estimation(Gutmann, 2012)[42]&#xFF0C;&#x4EE3;&#x66FF;&#x539F;&#x9884;&#x6D4B;&#x76EE;&#x6807;&#x8BCD;&#x7684;&#x6982;&#x7387;&#xFF0C;&#x52A0;&#x5FEB;&#x4E86;&#x7B97;&#x6CD5;&#x7684;&#x6536;&#x655B;&#x3002;&#x5728;&#x901F;&#x5EA6;&#x4E0A;&#x6BD4;CBOW&#x548C;skipgram&#x6709;&#x4E86;&#x8FDB;&#x4E00;&#x6B65;&#x6539;&#x5584;&#x3002;</li>
<li>(Mikolov, 2013)[23] &#x5728;skipgram&#x57FA;&#x7840;&#x4E0A;&#x5C1D;&#x8BD5;&#x4E86;NCE&#xFF0C;&#x5E76;&#x8FDB;&#x4E00;&#x6B65;&#x7B80;&#x5316;NCE&#x81F3;Negative Sampling&#x3002;&#x8BAD;&#x7EC3;&#x8FC7;&#x7A0B;&#x4E2D;&#x5F15;&#x5165;&#x4E86;&#x91C7;&#x6837;(subsampling)&#x6765;&#x51CF;&#x8F7B;&#x65E0;&#x610F;&#x4E49;&#x7684;&#x9AD8;&#x9891;&#x8BCD;&#x5BF9;&#x8BCD;&#x5411;&#x91CF;&#x5B66;&#x4E60;&#x5E26;&#x6765;&#x7684;&#x5F71;&#x54CD;&#x3002;&#x5F15;&#x5165;&#x77ED;&#x8BED;&#x8BC6;&#x522B;&#xFF0C;&#x89E3;&#x51B3;&#x5148;&#x524D;&#x8BCD;&#x5411;&#x91CF;&#x8BAD;&#x7EC3;&#x5355;&#x4F4D;&#x4E3A;unigram&#x5BFC;&#x81F4;&#x4E13;&#x540D;&#x88AB;&#x6253;&#x6563;&#x7684;&#x95EE;&#x9898;&#x3002;&#x8FD9;&#x4E9B;&#x7279;&#x6027;&#x540C;&#x6837;&#x53EF;&#x4EE5;&#x7528;&#x5728;CBOW&#x6A21;&#x578B;&#x3002;</li>
</ol>
<p>&#x4E0A;&#x9762;&#x7F57;&#x5217;&#x4E86;&#x8FD1;&#x4E9B;&#x5E74;&#x8BCD;&#x5411;&#x91CF;&#x6280;&#x672F;&#x53D1;&#x5C55;&#x7684;&#x6BD4;&#x8F83;&#x91CD;&#x8981;&#x7684;&#x6587;&#x7AE0;&#xFF0C;&#x5E0C;&#x671B;&#x80FD;&#x5E2E;&#x52A9;&#x8BFB;&#x8005;&#x4E86;&#x89E3;&#x5BF9;&#x8BCD;&#x5411;&#x91CF;&#x7684;&#x53D1;&#x5C55;&#x8FC7;&#x7A0B;&#x3002;&#x8FD1;&#x4E24;&#x5E74;&#x4E5F;&#x6709;&#x4E00;&#x4E9B;&#x6BD4;&#x8F83;&#x597D;&#x7684;&#x5DE5;&#x4F5C;&#xFF0C;<br>a). &#x901A;&#x8FC7;&#x5728;&#x76EE;&#x6807;&#x51FD;&#x6570;&#x4E2D;&#xFF0C;&#x5F15;&#x5165;&#x8BCD;&#x5178;&#x4E2D;&#x8BCD;&#x4E49;&#x5173;&#x7CFB;&#xFF08;&#x540C;&#x4E49;&#x8BCD;&#x3001;&#x8BCD;&#x7684;&#x4E0A;&#x4E0B;&#x4F4D;&#xFF09;&#x8FDB;&#x884C;&#x8054;&#x5408;&#x8BAD;&#x7EC3;(Yu M, 2014)[46]&#x3002;<br>b). &#x5F15;&#x5165;&#x53CC;&#x8BED;&#x8BED;&#x6599;&#xFF0C;&#x5728;&#x4E0D;&#x540C;&#x8BED;&#x79CD;&#x7684;&#x8BED;&#x6599;&#x65E0;&#x76D1;&#x7763;&#x5F97;&#x5230;&#x8BCD;&#x5411;&#x91CF;&#xFF0C;&#x5229;&#x7528;&#x4E0D;&#x540C;&#x8BED;&#x6599;&#x95F4;&#x8BCD;&#x5BF9;&#x7684;&#x5173;&#x7CFB;&#x8FDB;&#x884C;&#x5178;&#x578B;&#x76F8;&#x5173;&#x5206;&#x6790;(canonical correction analysis)&#xFF0C;&#x5C06;&#x8BCD;&#x5411;&#x91CF;&#x6620;&#x5C04;&#x5230;&#x4E00;&#x4E2A;&#x65B0;&#x7684;&#x7A7A;&#x95F4;(Faruqui, 2014)[47]&#x3002;<br>c). &#x5728;&#x5404;&#x6A21;&#x578B;&#x5F97;&#x5230;&#x7684;&#x8BCD;&#x5411;&#x91CF;&#x57FA;&#x7840;&#x4E0A;&#xFF0C;&#x5F15;&#x5165;&#x8BCD;&#x5178;&#x8D44;&#x6E90;&#x8FDB;&#x884C;&#x589E;&#x5F3A;(Faruqui M, 2014)[43]&#x3002;<br>d). &#x5C06;&#x7F51;&#x7EDC;&#x7ED3;&#x6784;&#x8F6C;&#x4E3A;&#x5C40;&#x90E8;&#x7EBF;&#x6027;&#x7ED3;&#x6784;&#x5B66;&#x4E60;&#x8282;&#x70B9;&#x7684;embedding&#xFF0C;&#x6839;&#x636E;&#x8F6C;&#x5316;&#x65B9;&#x5F0F;&#x53EF;&#x4EE5;&#x5206;&#x4E3A;&#x6DF1;&#x5EA6;&#x4F18;&#x5148;&#x7248;&#x672C;DeepWalk(Perozzi, 2014)[44]&#xFF0C;&#x5E7F;&#x5EA6;&#x4F18;&#x5148;&#x7248;&#x672C;LINE(Tang, 2015)[45]&#x3002;<br>e). &#x8FDB;&#x4E00;&#x6B65;&#xFF0C;&#x5728;&#x5B66;&#x4E60;&#x7F51;&#x7EDC;&#x7ED3;&#x6784;embedding&#x7684;&#x57FA;&#x7840;&#x4E0A;&#x5F15;&#x5165;&#x4E86;&#x534A;&#x76D1;&#x7763;&#x5B66;&#x4E60;&#xFF0C;&#x5E76;&#x7ED9;&#x51FA;&#x4E86;transductive&#x6846;&#x67B6;&#xFF08;DeepWalk&amp;LINE&#xFF09;&#x548C;inductive&#x6846;&#x67B6;&#x3002;(Yang Z, 2016)[46].</p>
<p>&#x5982;&#x4F55;&#x8861;&#x91CF;&#x8BCD;&#x5411;&#x91CF;&#x7684;&#x8BED;&#x8A00;&#x5B66;&#x7279;&#x6027;&#xFF0C;&#x6765;&#x535A;&#x58EB;&#x7684;&#x6BD5;&#x4E1A;&#x8BBA;&#x6587;(2016)[1]&#x603B;&#x7ED3;&#x4E86;8&#x4E2A;&#x8BC4;&#x4EF7;&#x6307;&#x6807;</p>
<ul>
<li>&#x8BCD;&#x4E49;&#x76F8;&#x5173;&#x6027;(ws): WordSim353&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x8BCD;&#x5BF9;&#x8BED;&#x4E49;&#x6253;&#x5206;&#x3002;</li>
<li>&#x540C;&#x4E49;&#x8BCD;&#x68C0;&#x6D4B;(tfl): TOEFL&#x6570;&#x636E;&#x96C6;&#xFF0C;80&#x4E2A;&#x5355;&#x9009;&#x9898;&#x3002;&#x51C6;&#x786E;&#x7387;&#x8BC4;&#x4EF7;</li>
<li>&#x5355;&#x8BCD;&#x8BED;&#x4E49;&#x7C7B;&#x6BD4;(sem): 9000&#x4E2A;&#x95EE;&#x9898;&#x3002;queen-king+man=women&#x3002;&#x51C6;&#x786E;&#x7387;</li>
<li>&#x5355;&#x8BCD;&#x53E5;&#x6CD5;&#x7C7B;&#x6BD4;(syn): 1W&#x4E2A;&#x95EE;&#x9898;&#x3002;dancing-dance+predict=predicting&#x3002;&#x51C6;&#x786E;&#x7387;<br>&#x8BCD;&#x5411;&#x91CF;&#x7528;&#x4F5C;&#x7279;&#x5F81;</li>
<li>&#x57FA;&#x4E8E;&#x5E73;&#x5747;&#x8BCD;&#x5411;&#x91CF;&#x7684;&#x6587;&#x672C;&#x5206;&#x7C7B;(avg): IMDB&#x6570;&#x636E;&#x96C6;&#xFF0C;Logistic&#x5206;&#x7C7B;&#x3002;&#x51C6;&#x786E;&#x7387;&#x8BC4;&#x4EF7;</li>
<li>&#x547D;&#x540D;&#x5B9E;&#x4F53;&#x8BC6;&#x522B;(ner): CoNLL03&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x4F5C;&#x4E3A;&#x73B0;&#x6709;&#x7CFB;&#x7EDF;&#x7684;&#x989D;&#x5916;&#x7279;&#x5F81;&#x3002;F1&#x503C;<br>&#x8BCD;&#x5411;&#x91CF;&#x7528;&#x4F5C;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x6A21;&#x578B;&#x7684;&#x521D;&#x59CB;&#x503C;</li>
<li>&#x57FA;&#x4E8E;&#x5377;&#x79EF;&#x7684;&#x6587;&#x672C;&#x5206;&#x7C7B;(cnn): &#x65AF;&#x5766;&#x798F;&#x60C5;&#x611F;&#x6811;&#x5E93;&#x6570;&#x636E;&#x96C6;&#xFF0C;&#x8BCD;&#x5411;&#x91CF;&#x4E0D;&#x56FA;&#x5B9A;&#x3002;&#x51C6;&#x786E;&#x7387;</li>
<li>&#x8BCD;&#x6027;&#x6807;&#x6CE8;(pos): &#x534E;&#x5C14;&#x8857;&#x65E5;&#x62A5;&#x6570;&#x636E;&#x96C6;&#xFF0C;Collobert&#x7B49;&#x4EBA;&#x63D0;&#x51FA;&#x7684;NN&#x3002;&#x51C6;&#x786E;&#x7387;</li>
</ul>
<p>&#x8BBA;&#x6587;&#x4E2D;&#x5BF9;&#x5404;&#x6A21;&#x578B;&#x8FDB;&#x884C;&#x4E86;&#x6A2A;&#x5411;&#x5B9E;&#x9A8C;&#x6BD4;&#x8F83;&#xFF0C;&#x63A8;&#x8350;&#x9605;&#x8BFB;&#x3002;</p>
<h4 id="word2vec&#x6E90;&#x7801;&#x53CA;&#x6269;&#x5C55;"><a href="#word2vec&#x6E90;&#x7801;&#x53CA;&#x6269;&#x5C55;" class="headerlink" title="word2vec&#x6E90;&#x7801;&#x53CA;&#x6269;&#x5C55;"></a>word2vec&#x6E90;&#x7801;&#x53CA;&#x6269;&#x5C55;</h4><p>&#x4F5C;&#x8005; Tomas Mikolov (<a href="https://www.linkedin.com/in/tomas-mikolov-59831188" target="_blank" rel="external">Linked In</a>) &#x76EE;&#x524D;&#x5728;facebook<br>word2vec: <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="external">&#x4E0B;&#x8F7D;&#x5730;&#x5740;</a>, <a href="https://code.google.com/archive/p/word2vec/source/default/commits" target="_blank" rel="external">&#x66F4;&#x65B0;&#x65E5;&#x5FD7;</a><br>&#x7B14;&#x8005;&#x5BF9;&#x6838;&#x5FC3;&#x4EE3;&#x7801;word2vec.c&#x8FDB;&#x884C;&#x4E86;&#x6CE8;&#x91CA;&#xFF08;&#x6309;&#xFF1A;&#x53EF;&#x80FD;&#x662F;&#x5E02;&#x9762;&#x4E0A;&#x6700;&#x8BE6;lao&#x7EC6;dao&#x7684;&#x6CE8;&#x91CA;&#xFF09;&#xFF0C;&#x589E;&#x52A0;&#x4E86;&#x4E24;&#x4E2A;&#x5C0F;&#x5DE5;&#x5177;&#xFF0C;&#x89C1;<a href="https://github.com/qiugen/word2vec_comments" target="_blank" rel="external">&#x3010;github&#x4ED3;&#x5E93;&#x3011;</a>&#xFF0C;txt2bin(&#x660E;&#x6587;&#x8F6C;&#x4E8C;&#x8FDB;&#x5236;)&#x548C;distance_search&#xFF08;&#x9012;&#x5F52;&#x67E5;&#x627E;&#x8FD1;&#x90BB;&#x5411;&#x91CF;&#xFF09;&#x3002;</p>
<p>[&#x66F4;&#x591A;&#x6269;&#x5C55;]&#x5BF9;word2vec&#x7684;&#x5177;&#x4F53;&#x5B9E;&#x73B0;&#x8FDB;&#x884C;&#x4F18;&#x5316;&#x53EF;&#x4EE5;&#x53C2;&#x8003;<a href="https://github.com/BIDData/BIDMach" target="_blank" rel="external">&#x201C;BIDMatch&#x201D;</a>(canny J, 2015)[48]&#x9AD8;&#x6027;&#x80FD;&#x673A;&#x5668;&#x5B66;&#x4E60;&#x5E93;&#x7684;GPU&#x7248;&#x672C;&#xFF0C;&#x89C1; <a href="https://github.com/BIDData/BIDMach/blob/master/src/main/scala/BIDMach/networks/Word2Vec.scala" target="_blank" rel="external">&#x3010;github&#x4ED3;&#x5E93;&#x3011;</a>&#x3002; (Ji S, 2016)[49]&#x5C06;&#x8BA1;&#x7B97;&#x5411;&#x91CF;&#x5185;&#x79EF;&#x8F6C;&#x4E3A;&#x77E9;&#x9635;&#x8FD0;&#x7B97;&#xFF0C;&#x4F7F;&#x5F97;&#x8BA1;&#x7B97;&#x4ECE;L1 level BLAS&#x8F6C;&#x4E3A;L3 level BLAS&#xFF0C;&#x5E76;&#x5B9E;&#x73B0;&#x4E86;&#x5206;&#x5E03;&#x5F0F;&#x7248;&#x672C;&#xFF0C;&#x652F;&#x6301;&#x589E;&#x52A0;&#x8BA1;&#x7B97;&#x8282;&#x70B9;&#x52A0;&#x901F;&#x8BCD;&#x5411;&#x91CF;&#x8BAD;&#x7EC3;&#xFF0C;&#x89C1;<a href="https://github.com/IntelLabs/pWord2Vec" target="_blank" rel="external">&#x3010;github&#x4ED3;&#x5E93;&#x3011;</a>&#x3002;</p>
<p>&#x7B14;&#x8005;&#x6309;&#xFF1A;&#x672C;&#x6587;&#x5B9A;&#x4F4D;&#x4E3A;&#x8BBA;&#x6587;&#x9605;&#x8BFB;&#x6574;&#x7406;&#x7B14;&#x8BB0;&#xFF0C;&#x524D;&#x524D;&#x540E;&#x540E;&#x5386;&#x65F6;&#x6570;&#x6708;&#x3002;&#x5982;&#x679C;&#x9519;&#x6F0F;&#x4E86;&#x77E5;&#x8BC6;&#xFF0C;&#x8BF7;&#x5728;&#x8BC4;&#x8BBA;&#x91CC;&#x6307;&#x51FA;&#xFF0C;&#x8C22;&#x8C22;&#x3002;&#x5168;&#x6587;&#x6CA1;&#x6709;&#x516C;&#x5F0F;&#xFF0C;&#x8BFB;&#x8005;&#x5982;&#x679C;&#x80FD;&#x8010;&#x5FC3;&#x770B;&#x5230;&#x8FD9;&#x91CC;&#xFF0C;&#x6211;&#x4F1A;&#x5F88;&#x5F00;&#x5FC3;&#x3002;&#x63A5;&#x4E0B;&#x6765;&#x4F1A;&#x5728;&#x4E1A;&#x4F59;&#x65F6;&#x95F4;&#x5BF9;word2vec&#x8FDB;&#x884C;&#x4EE3;&#x7801;&#x8D70;&#x8BFB;&#xFF0C;&#x7136;&#x540E;&#x5728;&#x8FD9;&#x4E2A;&#x57FA;&#x7840;&#x4E0A;&#x505A;&#x4E9B;&#x8BCD;&#x4E49;&#x76F8;&#x5173;&#x7684;&#x5C0F;&#x5B9E;&#x9A8C;&#xFF0C;&#x5E0C;&#x671B;&#x5F97;&#x5230;&#x6709;&#x610F;&#x601D;&#x7684;&#x7ED3;&#x679C;&#x3002;</p>
<p>[1]&#x57FA;&#x4E8E;&#x795E;&#x7ECF;&#x7F51;&#x7EDC;&#x7684;&#x8BCD;&#x548C;&#x6587;&#x6863;&#x8BED;&#x4E49;&#x5411;&#x91CF;&#x8868;&#x793A;&#x65B9;&#x6CD5;&#x7814;&#x7A76;&#xFF0C;&#x6765;&#x65AF;&#x60DF;&#xFF0C;&#x4E2D;&#x79D1;&#x9662;&#x81EA;&#x52A8;&#x5316;&#x7814;&#x7A76;&#x6240;, 2016&#xFF0C;&#x6587;&#x7AE0;&#x5730;&#x5740;&#xFF1A;<a href="http://pan.baidu.com/s/1qYbEByk" target="_blank" rel="external">&#x767E;&#x5EA6;&#x7F51;&#x76D8;</a><br>[2] Daniel D Lee and H Sebastian Seung. Algorithms for non-negative matrix  actorization. In Advances in neural information processing systems, pages 556&#x2013;562, 2001.<br>[3] Chih-Jen Lin. Projected gradient methods for nonnegative matrix factorization. Neural computation, 19(10):2756&#x2013;2779, 2007.<br>[4] Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Ungar. Multi-view learning of word embeddings via cca. In Advances in Neural Information Processing Systems, pages 199&#x2013;207, 2011.<br>[5] Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Ungar. Eigenwords: Spectral word embeddings. The Journal of Machine Learning Research, 16, 2015.<br>[6] R&#xE9;mi Lebret and Ronan Collobert. Word embeddings through hellinger pca. EACL 2014, page 482, 2014.<br>[7] Thomas K Landauer, Peter W Foltz, and Darrell Laham. An introduction to latent semantic analysis. Discourse processes, 25(2-3):259&#x2013;284, 1998.<br>[8] Jeffrey Pennington, Richard Socher, and Christopher D Manning. GloVe : Global Vectors for Word Representation. In Proceedings of the Empiricial Methods in Natural Language Processing, 2014.<br>[9] Fernando Pereira, Naftali Tishby, and Lillian Lee. Distributional clustering of english words. In Proceedings of the 31st annual meeting on Association for Computational Linguistics, pages 183&#x2013;190, 1993.<br>[10] Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. Class-based n-gram models of natural language. Computational linguistics, 18(4):467&#x2013;479, 1992.<br>[11] Robert M. Bell and Yehuda Koren. Lessons from the netflix prize challenge. SIGKDD Explor. Newsl., 9:75&#x2013;79, 2007.<br>[12]Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for semi-supervised learning.<br>[13] Wei Xu and Alex Rudnicky. Can artificial neural networks learn language models? In Sixth International Conference on Spoken Language Processing, 2000.<br>[14] Yoshua Bengio, R&#xE9;jean Ducharme, and Pascal Vincent. A neural probabilistic language model. In Advances in Neural Information Processing Systems, pages 932&#x2013;938, 2001.<br>[15] Yoshua Bengio, R&#xE9;jean Ducharme, Pascal Vincent, and Christian Jauvin. A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3:1137&#x2013;1155, 2003.<br>[16] Andriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language modelling. In Proceedings of the 24th international conference on Machine learning, pages 641&#x2013;648, 2007.<br>[17] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pages 1081&#x2013;1088, 2008.<br>[18] Andriy Mnih and Koray Kavukcuoglu. Learning word embeddings efficiently with noise-contrastive estimation. In Advances in Neural Information Processing Systems, pages 2265&#x2013;2273, 2013.<br>[19] Tom&#xE1;&#x161; Mikolov. Statistical language models based on neural networks. PhD thesis, Brno University of Technology, 2012.<br>[20] Tomas Mikolov, Martin Karafi&#xE1;t, Lukas Burget, Jan Cernock&#x1EF3;, and Sanjeev Khudanpur. Recurrent neural network based language model. In INTERSPEECH 2010, 11th Annual Conference of the International Speech Communication Association, pages 1045&#x2013;1048, 2010.<br>[21]Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. International Conference on Machine Learning, 2008<br>[22]Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. International Conference on Learning Representations Workshop Track, 2013.<br>[23]Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111&#x2013;3119, 2013<br>[24] Turian J, Ratinov L, Bengio Y. Word representations: a simple and general method for semi-supervised learning[C]// ACL 2010, Proceedings of the, Meeting of the Association for Computational Linguistics, July 11-16, 2010, Uppsala, Sweden. 2010:780-1.<br>[25] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. In Advances in Neural Information Processing Systems, pages 2177&#x2013;2185, 2014.<br>[26] Yitan Li, Linli Xu, Fei Tian, Liang Jiang, Xiaowei Zhong, and Enhong Chen. Word embedding revisited: A new representation learning and explicit matrix factorization perspective.<br>[27] Pennington J, Socher R, Manning C. Glove: Global Vectors for Word Representation[C]// Conference on Empirical Methods in Natural Language Processing. 2014.<br>[28]Ronan Collobert, Jason Weston, L&#xB4;eon Bottou,Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost) from Scratch. JMLR, 12:2493&#x2013;2537<br>[29] Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recursive neural networks for morphology. CoNLL-2013<br>[30] Omer Levy, Yoav Goldberg, and Israel Ramat-Gan. 2014. Linguistic regularities in sparse and explicit word representations. CoNLL-2014.<br>[31] Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41.<br>[32] Yogatama D, Faruqui M, Dyer C, et al. Learning Word Representations with Hierarchical Sparse Coding[J]. Eprint Arxiv, 2014.<br>[33] Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41.<br>[34] Bellegarda J R. A latent semantic analysis framework for large-Span language modeling.[C]// European Conference on Speech Communication and Technology, Eurospeech 1997, Rhodes, Greece, September. 1997.<br>[35] R. Kneser and H. Ney. Improved backing-off for m-gram language modeling. In International Conference on Acoustics, Speech and Signal Processing, pages 181&#x2013;184, 1995.<br>[36] T.R. Niesler, E.W.D. Whittaker, and P.C. Woodland. Comparison of part-of-speech and automatically derived category-based language models for speech recognition. In International Conference on Acoustics, Speech and Signal Processing, pages 177&#x2013;180, 1998.<br>[37] Deerwester S, Dumais S T, Furnas G W, et al. Indexing by latent semantic analysis[J]. Journal of the Association for Information Science and Technology, 1990, 41(6):391-407.<br>[38] Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. Neural Computation, 14, 1711&#x2013;1800.<br>[39] FredericMorin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Robert G. Cowell and Zoubin Ghahramani, editors, AISTATS&#x2019;05, pages 246&#x2013;252, 2005<br>[40] Goodman, J. (2001b). Classes for fast maximum entropy training. In International Conference on Acoustics, Speech, and Signal Processing, Utah.<br>[41] M.U. Gutmann and A. Hyv&#xA8;arinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. Journal of Machine Learning Research, 13:307&#x2013;361, 2012.<br>[42] Gutmann M U, Hyv&amp;#, Rinen A. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics[J]. Journal of Machine Learning Research, 2012, 13(1):307-361.<br>[43] Faruqui M, Dodge J, Jauhar S K, et al. Retrofitting Word Vectors to Semantic Lexicons[J]. Eprint Arxiv, 2014.<br>[44] Perozzi B, Alrfou R, Skiena S. DeepWalk: online learning of social representations[J]. Eprint Arxiv, 2014:701-710.<br>[45] Tang J, Qu M, Wang M, et al. LINE: Large-scale Information Network Embedding[J]. 2015.<br>[46] Yu M, Dredze M. Improving Lexical Embeddings with Semantic Knowledge[C]// Meeting of the Association for Computational Linguistics. 2014:545-550.<br>[47] Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. In Proceedings of EACL<br>[48] Canny J, Zhao H, Jaros B, et al. Machine learning at the limit[C]// IEEE International Conference on Big Data. IEEE Computer Society, 2015:233-242.<br>[49] Ji S, Satish N, Li S, et al. Parallelizing Word2Vec in Shared and Distributed Memory[J]. 2016.<br>[50]Yang Z, Cohen W W, Salakhutdinov R. Revisiting Semi-Supervised Learning with Graph Embeddings[J]. 2016.</p>
<p>Author: ShawnXiao@baidu</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="https://mlnote.com/2016/12/18/Reading-Notes-of-Word-Embedding/" data-id="cj1auuqr600071slpmw7abwbc" class="article-share-link">分享到</a><div class="tags"><a href="/tags/neural-networks/">neural networks</a><a href="/tags/word-embedding/">word embedding</a></div><div class="post-nav"><a href="/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/" class="pre">浅谈Batch Normalization及其Caffe实现</a><a href="/2016/12/10/SyntaxNet Neural-Models-of-Syntax/" class="next">简介语法分析开源神经网络SyntaxNet</a></div><div data-thread-key="2016/12/18/Reading-Notes-of-Word-Embedding/" data-title="Word Embedding札记" data-url="https://mlnote.com/2016/12/18/Reading-Notes-of-Word-Embedding/" class="ds-share flat"><div class="ds-share-inline"><ul class="ds-share-icons-16"><li data-toggle="ds-share-icons-more"><a href="javascript:void(0);" class="ds-more">分享到：</a></li><li><a href="javascript:void(0);" data-service="weibo" class="ds-weibo">微博</a></li><li><a href="javascript:void(0);" data-service="qzone" class="ds-qzone">QQ空间</a></li><li><a href="javascript:void(0);" data-service="qqt" class="ds-qqt">腾讯微博</a></li><li><a href="javascript:void(0);" data-service="wechat" class="ds-wechat">微信</a></li></ul><div class="ds-share-icons-more"></div></div></div><div data-thread-key="2016/12/18/Reading-Notes-of-Word-Embedding/" data-title="Word Embedding札记" data-url="https://mlnote.com/2016/12/18/Reading-Notes-of-Word-Embedding/" data-author-key="1" class="ds-thread"></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.baidu.com/baidu" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="search" name="word" maxlength="20" placeholder="Search"/><input type="hidden" name="si" value="https://mlnote.com"/><input name="tn" type="hidden" value="bds"/><input name="cl" type="hidden" value="3"/><input name="ct" type="hidden" value="2097152"/><input name="s" type="hidden" value="on"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/其他/">其他</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/gbdt/" style="font-size: 15px;">gbdt</a> <a href="/tags/user-targeting/" style="font-size: 15px;">user targeting</a> <a href="/tags/Batch-Normalization/" style="font-size: 15px;">Batch Normalization</a> <a href="/tags/Caffe/" style="font-size: 15px;">Caffe</a> <a href="/tags/neural-networks/" style="font-size: 15px;">neural-networks</a> <a href="/tags/learning-to-rank/" style="font-size: 15px;">learning to rank</a> <a href="/tags/query-rewriting/" style="font-size: 15px;">query rewriting</a> <a href="/tags/semantic-matching/" style="font-size: 15px;">semantic matching</a> <a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/error-correction/" style="font-size: 15px;">error correction</a> <a href="/tags/spelling-correction/" style="font-size: 15px;">spelling correction</a> <a href="/tags/neural-networks/" style="font-size: 15px;">neural networks</a> <a href="/tags/word-embedding/" style="font-size: 15px;">word embedding</a> <a href="/tags/SyntaxNet/" style="font-size: 15px;">SyntaxNet</a> <a href="/tags/CRF/" style="font-size: 15px;">CRF</a> <a href="/tags/fastBDT/" style="font-size: 15px;">fastBDT</a> <a href="/tags/advertisement/" style="font-size: 15px;">advertisement</a> <a href="/tags/lightGBM/" style="font-size: 15px;">lightGBM</a> <a href="/tags/RBMs/" style="font-size: 15px;">RBMs</a> <a href="/tags/gbRank/" style="font-size: 15px;">gbRank</a> <a href="/tags/logisticRank/" style="font-size: 15px;">logisticRank</a> <a href="/tags/xgboost/" style="font-size: 15px;">xgboost</a> <a href="/tags/gradient-boosting-framework/" style="font-size: 15px;">gradient boosting framework</a> <a href="/tags/huber/" style="font-size: 15px;">huber</a> <a href="/tags/LST/" style="font-size: 15px;">LST</a> <a href="/tags/LAD/" style="font-size: 15px;">LAD</a> <a href="/tags/classify/" style="font-size: 15px;">classify</a> <a href="/tags/model/" style="font-size: 15px;">model</a> <a href="/tags/bayes/" style="font-size: 15px;">bayes</a> <a href="/tags/code-mannar/" style="font-size: 15px;">code mannar</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a> <a href="/tags/mathjex-公式/" style="font-size: 15px;">mathjex 公式</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/04/09/Reading-Notes-of-Error-Correction/">一些纠错相关的论文笔记</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/20/Neural-Network-Batch-Normalization-and-Caffe-Code/">浅谈Batch Normalization及其Caffe实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/18/Reading-Notes-of-Word-Embedding/">Word Embedding札记</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/12/10/SyntaxNet Neural-Models-of-Syntax/">简介语法分析开源神经网络SyntaxNet</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/11/18/The-realization-of-GBDT-in-LightGBM-and-FastDBT/">简述FastDBT和LightGBM中GBDT的实现</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/29/xgboost-code-review-with-paper/">XGboost核心源码阅读</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/05/a-guide-to-xgboost-A-Scalable-Tree-Boosting-System/">XGboost: A Scalable Tree Boosting System论文及源码导读</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/10/02/gradient-boosting-decision-tree-2/">Gradient Boosting Decision Tree[下篇]</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/24/gradient-boosting-decision-tree-1/">Gradient Boosting Decision Tree[上篇]</a></li><li class="post-list-item"><a class="post-list-link" href="/2016/09/18/gbRank-logsitRank-from-up-to-bottom/">gbRank & logsitcRank自顶向下</a></li></ul></div><div class="widget"><div class="comments-title"><i class="fa fa-comment-o"> 最近评论</i></div><div data-num-items="5" data-show-avatars="0" data-show-time="1" data-show-admin="0" data-excerpt-length="32" data-show-title="1" class="ds-recent-comments"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">© <a href="/." rel="nofollow">水滴石穿.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script>var duoshuoQuery = {short_name:'qiugen'};
(function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0]
        || document.getElementsByTagName('body')[0]).appendChild(ds);
})();
</script><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "//hm.baidu.com/hm.js?ceebf89bb3d2a3b32aff294e42be4ed7";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
  })();
</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>